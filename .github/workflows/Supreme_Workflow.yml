name: Bug Bounty Hunter Suite

on:
  schedule:
    - cron: '0 18 * * *'
  workflow_dispatch:
    inputs:
      domain:
        description: 'Target domain to scan'
        required: true
        default: 'reddit.com'

permissions:
  contents: write

jobs:
  # Job 1: Subdomain Discovery
  subdomain-discovery:
    runs-on: ubuntu-latest
    outputs:
      subdomains-file: ${{ steps.set-outputs.outputs.subdomains-file }}
    env:
      DOMAIN: ${{ github.event.inputs.domain || 'reddit.com' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v4
        with:
          go-version: '1.21'

      - name: Install Tools
        run: |
          go install -v github.com/projectdiscovery/subfinder/v2/cmd/subfinder@latest
          go install -v github.com/projectdiscovery/httpx/cmd/httpx@latest

      - name: Prepare workspace
        run: |
          mkdir -p results/$DOMAIN
          mkdir -p tmp/$DOMAIN

      - name: Run subfinder
        id: subfinder
        run: |
          echo "[*] Running subfinder for $DOMAIN"
          subfinder -d "$DOMAIN" -silent -o results/$DOMAIN/subdomains.txt || true
          sort -u results/$DOMAIN/subdomains.txt -o results/$DOMAIN/subdomains.txt || true
          echo "SUBDOMAINS_COUNT=$(wc -l < results/$DOMAIN/subdomains.txt || echo 0)" >> $GITHUB_ENV
          echo "[*] Subdomains found: $(cat results/$DOMAIN/subdomains.txt || echo '(none)')"
          
          # Set output for other jobs
          SUBDOMAINS_FILE="results/$DOMAIN/subdomains.txt"
          echo "subdomains-file=$SUBDOMAINS_FILE" >> $GITHUB_OUTPUT

      - name: Upload subdomains artifact
        uses: actions/upload-artifact@v4
        with:
          name: subdomains-${{ env.DOMAIN }}
          path: results/${{ env.DOMAIN }}/subdomains.txt
          retention-days: 1

  # Job 2: Directory Bruteforcing with Gobuster
  gobuster-scan:
    runs-on: ubuntu-latest
    needs: subdomain-discovery
    outputs:
      gobuster-results: ${{ steps.set-outputs.outputs.gobuster-results }}
    env:
      DOMAIN: ${{ github.event.inputs.domain || 'reddit.com' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download subdomains artifact
        uses: actions/download-artifact@v4
        with:
          name: subdomains-${{ env.DOMAIN }}
          path: results/${{ env.DOMAIN }}/

      - name: Install Gobuster
        run: |
          sudo apt-get update
          sudo apt-get install -y gobuster

      - name: Prepare workspace
        run: |
          mkdir -p results/$DOMAIN/gobuster
          mkdir -p tmp/$DOMAIN

      - name: Run Gobuster on main domain
        run: |
          echo "[*] Running Gobuster on $DOMAIN"
          gobuster dir -u https://$DOMAIN -w /usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt \
            -x php,asp,aspx,jsp,html,htm,js,json,txt,log,bak,backup,old,zip,tar,gz,sql,conf,config,yml,yaml,env \
            -t 50 -r -e -k -o results/$DOMAIN/gobuster/main-domain.txt || true

      - name: Run Gobuster on subdomains (top 10)
        run: |
          echo "[*] Running Gobuster on top 10 subdomains"
          head -n 10 results/$DOMAIN/subdomains.txt | while read sub; do
            sub="$(echo "$sub" | tr -d '\r')"
            [ -z "$sub" ] && continue
            echo "[+] Scanning $sub"
            gobuster dir -u https://$sub -w /usr/share/wordlists/dirbuster/directory-list-2.3-small.txt \
              -x php,asp,aspx,jsp,html,htm,js,json,txt,log,bak,backup,old,zip,tar,gz,sql,conf,config,yml,yaml,env \
              -t 30 -r -e -k -o results/$DOMAIN/gobuster/sub-${sub}.txt || true
          done

      - name: Combine Gobuster results
        id: combine-gobuster
        run: |
          mkdir -p results/$DOMAIN/combined
          cat results/$DOMAIN/gobuster/*.txt | grep -E "^https?://.+" | sort -u > results/$DOMAIN/combined/gobuster-results.txt || true
          
          # Set output for other jobs
          GOBUSTER_RESULTS="results/$DOMAIN/combined/gobuster-results.txt"
          echo "gobuster-results=$GOBUSTER_RESULTS" >> $GITHUB_OUTPUT
          
          echo "GOBUSTER_COUNT=$(wc -l < results/$DOMAIN/combined/gobuster-results.txt || echo 0)" >> $GITHUB_ENV
          echo "[*] Gobuster found $(wc -l < results/$DOMAIN/combined/gobuster-results.txt || echo 0) endpoints"

      - name: Upload gobuster artifact
        uses: actions/upload-artifact@v4
        with:
          name: gobuster-results-${{ env.DOMAIN }}
          path: results/${{ env.DOMAIN }}/combined/gobuster-results.txt
          retention-days: 1

  # Job 3: Directory Bruteforcing with Dirsearch
  dirsearch-scan:
    runs-on: ubuntu-latest
    needs: subdomain-discovery
    outputs:
      dirsearch-results: ${{ steps.set-outputs.outputs.dirsearch-results }}
    env:
      DOMAIN: ${{ github.event.inputs.domain || 'reddit.com' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download subdomains artifact
        uses: actions/download-artifact@v4
        with:
          name: subdomains-${{ env.DOMAIN }}
          path: results/${{ env.DOMAIN }}/

      - name: Install Dirsearch
        run: |
          git clone https://github.com/maurosoria/dirsearch.git
          cd dirsearch
          pip3 install -r requirements.txt

      - name: Prepare workspace
        run: |
          mkdir -p results/$DOMAIN/dirsearch
          mkdir -p tmp/$DOMAIN

      - name: Run Dirsearch on main domain
        run: |
          echo "[*] Running Dirsearch on $DOMAIN"
          python3 dirsearch/dirsearch.py -u https://$DOMAIN -e php,asp,aspx,jsp,html,htm,js,json,txt,log,bak,backup,old,zip,tar,gz,sql,conf,config,yml,yaml,env \
            -t 30 -r --random-agent -f -x 400,403,404 -o results/$DOMAIN/dirsearch/main-domain.json || true

      - name: Run Dirsearch on subdomains (top 10)
        run: |
          echo "[*] Running Dirsearch on top 10 subdomains"
          head -n 10 results/$DOMAIN/subdomains.txt | while read sub; do
            sub="$(echo "$sub" | tr -d '\r')"
            [ -z "$sub" ] && continue
            echo "[+] Scanning $sub"
            python3 dirsearch/dirsearch.py -u https://$sub -e php,asp,aspx,jsp,html,htm,js,json,txt,log,bak,backup,old,zip,tar,gz,sql,conf,config,yml,yaml,env \
              -t 20 -r --random-agent -f -x 400,403,404 -o results/$DOMAIN/dirsearch/sub-${sub}.json || true
          done

      - name: Combine Dirsearch results
        id: combine-dirsearch
        run: |
          mkdir -p results/$DOMAIN/combined
          # Extract URLs from JSON files
          grep -h '"url":' results/$DOMAIN/dirsearch/*.json | sed 's/.*"url": *"\([^"]*\)".*/\1/' | sort -u > results/$DOMAIN/combined/dirsearch-results.txt || true
          
          # Set output for other jobs
          DIRSEARCH_RESULTS="results/$DOMAIN/combined/dirsearch-results.txt"
          echo "dirsearch-results=$DIRSEARCH_RESULTS" >> $GITHUB_OUTPUT
          
          echo "DIRSEARCH_COUNT=$(wc -l < results/$DOMAIN/combined/dirsearch-results.txt || echo 0)" >> $GITHUB_ENV
          echo "[*] Dirsearch found $(wc -l < results/$DOMAIN/combined/dirsearch-results.txt || echo 0) endpoints"

      - name: Upload dirsearch artifact
        uses: actions/upload-artifact@v4
        with:
          name: dirsearch-results-${{ env.DOMAIN }}
          path: results/${{ env.DOMAIN }}/combined/dirsearch-results.txt
          retention-days: 1

  # Job 4: JS File Enumeration and Analysis
  js-analysis:
    runs-on: ubuntu-latest
    needs: [gobuster-scan, dirsearch-scan]
    outputs:
      js-files: ${{ steps.set-outputs.outputs.js-files }}
      js-secrets: ${{ steps.set-outputs.outputs.js-secrets }}
    env:
      DOMAIN: ${{ github.event.inputs.domain || 'reddit.com' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download scan artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/
      
      - name: Install Tools
        run: |
          # Install TruffleHog
          curl -sSfL https://raw.githubusercontent.com/trufflesecurity/trufflehog/main/scripts/install.sh | sh -s -- -b /usr/local/bin
          
          # Install JQ for JSON processing
          sudo apt-get install -y jq
          
          # Install NodeJS for JS analysis tools
          curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash -
          sudo apt-get install -y nodejs
          
          # Install JS analysis tools
          npm install -g js-x-ray
          npm install -g nsp

      - name: Prepare workspace
        run: |
          mkdir -p results/$DOMAIN/js
          mkdir -p results/$DOMAIN/js/secrets
          mkdir -p tmp/$DOMAIN

      - name: Extract JS files from scan results
        id: extract-js
        run: |
          echo "[*] Extracting JS files from scan results"
          # Combine all scan results
          cat artifacts/gobuster-results-${{ env.DOMAIN }}/gobuster-results.txt artifacts/dirsearch-results-${{ env.DOMAIN }}/dirsearch-results.txt | sort -u > results/$DOMAIN/combined/all-endpoints.txt
          
          # Extract JS files
          grep -E "\.js(\?.*)?$" results/$DOMAIN/combined/all-endpoints.txt > results/$DOMAIN/js/js-files.txt || true
          
          # Set output for other jobs
          JS_FILES="results/$DOMAIN/js/js-files.txt"
          echo "js-files=$JS_FILES" >> $GITHUB_OUTPUT
          
          echo "JS_FILES_COUNT=$(wc -l < results/$DOMAIN/js/js-files.txt || echo 0)" >> $GITHUB_ENV
          echo "[*] Found $(wc -l < results/$DOMAIN/js/js-files.txt || echo 0) JS files"

      - name: Download and analyze JS files
        run: |
          echo "[*] Downloading and analyzing JS files"
          mkdir -p results/$DOMAIN/js/downloaded
          
          while read -r url; do
            url="$(echo "$url" | tr -d '\r')"
            [ -z "$url" ] && continue
            
            # Generate a safe filename
            safe=$(echo -n "$url" | sha1sum | awk '{print $1}')
            fn="results/$DOMAIN/js/downloaded/$safe.js"
            
            # Download the JS file
            if curl -L --max-time 15 --connect-timeout 5 -sS -k "$url" -o "$fn"; then
              echo "[+] Downloaded $url"
              
              # Analyze with js-x-ray
              js-x-ray "$fn" > "results/$DOMAIN/js/secrets/${safe}-xray.json" 2>/dev/null || true
              
              # Scan with TruffleHog
              trufflehog filesystem --path "$fn" --json >> "results/$DOMAIN/js/secrets/${safe}-trufflehog.json" 2>/dev/null || true
            else
              echo "[!] Failed to download $url"
            fi
          done < results/$DOMAIN/js/js-files.txt

      - name: Extract secrets from JS analysis
        id: extract-secrets
        run: |
          echo "[*] Extracting secrets from JS analysis"
          
          # Combine all secrets found
          mkdir -p results/$DOMAIN/js/combined
          
          # Extract secrets from TruffleHog results
          find results/$DOMAIN/js/secrets -name "*trufflehog.json" -exec cat {} \; | jq -r '.SourceMetadata|select(.Data|.Filesystem)' | jq -r '.Data.Filesystem.file' | sort -u > results/$DOMAIN/js/combined/js-secrets.txt || true
          
          # Extract interesting patterns from js-x-ray results
          find results/$DOMAIN/js/secrets -name "*xray.json" -exec cat {} \; | jq -r '.warnings[]?.warning' | grep -E "(api|key|token|secret|password|credential)" | sort -u >> results/$DOMAIN/js/combined/js-secrets.txt || true
          
          # Set output for other jobs
          JS_SECRETS="results/$DOMAIN/js/combined/js-secrets.txt"
          echo "js-secrets=$JS_SECRETS" >> $GITHUB_OUTPUT
          
          echo "JS_SECRETS_COUNT=$(wc -l < results/$DOMAIN/js/combined/js-secrets.txt || echo 0)" >> $GITHUB_ENV
          echo "[*] Found $(wc -l < results/$DOMAIN/js/combined/js-secrets.txt || echo 0) potential secrets in JS files"

      - name: Upload JS analysis artifact
        uses: actions/upload-artifact@v4
        with:
          name: js-analysis-${{ env.DOMAIN }}
          path: |
            results/${{ env.DOMAIN }}/js/js-files.txt
            results/${{ env.DOMAIN }}/js/combined/js-secrets.txt
          retention-days: 1

  # Job 5: Combine and Analyze Results
  combine-results:
    runs-on: ubuntu-latest
    needs: [gobuster-scan, dirsearch-scan, js-analysis]
    env:
      DOMAIN: ${{ github.event.inputs.domain || 'reddit.com' }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/

      - name: Prepare workspace
        run: |
          mkdir -p results/$DOMAIN/final
          mkdir -p previous/$DOMAIN

      - name: Combine all scan results
        run: |
          echo "[*] Combining all scan results"
          
          # Combine all scan results
          cat artifacts/gobuster-results-${{ env.DOMAIN }}/gobuster-results.txt artifacts/dirsearch-results-${{ env.DOMAIN }}/dirsearch-results.txt | sort -u > results/$DOMAIN/final/all-directories.txt
          
          # Add JS files
          cat artifacts/js-analysis-${{ env.DOMAIN }}/js-files.txt >> results/$DOMAIN/final/all-directories.txt
          sort -u results/$DOMAIN/final/all-directories.txt -o results/$DOMAIN/final/all-directories.txt
          
          echo "TOTAL_ENDPOINTS=$(wc -l < results/$DOMAIN/final/all-directories.txt || echo 0)" >> $GITHUB_ENV
          echo "[*] Combined $(wc -l < results/$DOMAIN/final/all-directories.txt || echo 0) total endpoints"

      - name: Filter for sensitive files
        run: |
          echo "[*] Filtering for sensitive files"
          
          # Create a list of sensitive files
          grep -Ei "\.(log|bak|backup|old|zip|tar|gz|sql|conf|config|yml|yaml|env|key|pem|p12|jks|keystore|htaccess|htpasswd|git|svn|ds_store|swp|tmp|temp)$" results/$DOMAIN/final/all-directories.txt > results/$DOMAIN/final/sensitive-files.txt || true
          
          # Add files with sensitive keywords in the path
          grep -Ei "(backup|config|credential|secret|key|token|password|admin|private|confidential|internal)" results/$DOMAIN/final/all-directories.txt >> results/$DOMAIN/final/sensitive-files.txt || true
          
          # Sort and deduplicate
          sort -u results/$DOMAIN/final/sensitive-files.txt -o results/$DOMAIN/final/sensitive-files.txt
          
          echo "SENSITIVE_FILES_COUNT=$(wc -l < results/$DOMAIN/final/sensitive-files.txt || echo 0)" >> $GITHUB_ENV
          echo "[*] Found $(wc -l < results/$DOMAIN/final/sensitive-files.txt || echo 0) potentially sensitive files"

      - name: Combine all secrets
        run: |
          echo "[*] Combining all secrets"
          
          # Copy JS secrets
          cp artifacts/js-analysis-${{ env.DOMAIN }}/js-secrets.txt results/$DOMAIN/final/js-secrets.txt || touch results/$DOMAIN/final/js-secrets.txt
          
          # Create a combined secrets file
          cat results/$DOMAIN/final/js-secrets.txt | sort -u > results/$DOMAIN/final/all-secrets.txt
          
          echo "TOTAL_SECRETS=$(wc -l < results/$DOMAIN/final/all-secrets.txt || echo 0)" >> $GITHUB_ENV
          echo "[*] Combined $(wc -l < results/$DOMAIN/final/all-secrets.txt || echo 0) total secrets"

      - name: Compare with previous results
        id: compare
        run: |
          echo "[*] Comparing with previous results"
          
          CUR="results/$DOMAIN/final/all-directories.txt"
          PREV="previous/$DOMAIN/all-directories.txt"
          
          mkdir -p previous/$DOMAIN
          : > results/$DOMAIN/final/new-endpoints.txt
          : > results/$DOMAIN/final/removed-endpoints.txt
          
          if [ -f "$PREV" ]; then
            comm -23 <(sort "$CUR") <(sort "$PREV") > results/$DOMAIN/final/new-endpoints.txt || true
            comm -13 <(sort "$CUR") <(sort "$PREV") > results/$DOMAIN/final/removed-endpoints.txt || true
            echo "NEW_COUNT=$(wc -l < results/$DOMAIN/final/new-endpoints.txt || echo 0)" >> $GITHUB_ENV
            echo "REMOVED_COUNT=$(wc -l < results/$DOMAIN/final/removed-endpoints.txt || echo 0)" >> $GITHUB_ENV
          else
            # First run: treat all as new and seed previous
            cp -f "$CUR" "$PREV" || true
            echo "NEW_COUNT=$(wc -l < $CUR || echo 0)" >> $GITHUB_ENV
            echo "REMOVED_COUNT=0" >> $GITHUB_ENV
            cp -f "$CUR" results/$DOMAIN/final/new-endpoints.txt || true
          fi

      - name: Save artifacts (results)
        uses: actions/upload-artifact@v4
        with:
          name: final-results-${{ env.DOMAIN }}-${{ github.run_id }}
          path: |
            results/${{ env.DOMAIN }}/final/
          retention-days: 7

      - name: Commit updated previous results
        run: |
          mkdir -p previous/$DOMAIN
          cp -f results/$DOMAIN/final/all-directories.txt previous/$DOMAIN/all-directories.txt || true
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add previous/$DOMAIN/all-directories.txt || true
          if git diff --staged --quiet; then
            echo "[*] No changes to previous endpoints to commit."
          else
            git commit -m "chore(recon): update previous endpoints for $DOMAIN"
            git push origin HEAD:main || echo "[!] Push failed"
          fi

      - name: Notify via webhook if changes detected
        if: env.NEW_COUNT != '0' || env.REMOVED_COUNT != '0'
        env:
          WEBHOOK_URL: ${{ secrets.REDDIT_DISCORD }}
        run: |
          NEW=$(cat results/$DOMAIN/final/new-endpoints.txt | wc -l || echo 0)
          REMOVED=$(cat results/$DOMAIN/final/removed-endpoints.txt | wc -l || echo 0)
          SENSITIVE=$(cat results/$DOMAIN/final/sensitive-files.txt | wc -l || echo 0)
          SECRETS=$(cat results/$DOMAIN/final/all-secrets.txt | wc -l || echo 0)
          
          NEW_EXAMPLES=$(head -n 10 results/$DOMAIN/final/new-endpoints.txt | sed ':a;N;$!ba;s/\n/\\n/g' || echo "None")
          SENSITIVE_EXAMPLES=$(head -n 10 results/$DOMAIN/final/sensitive-files.txt | sed ':a;N;$!ba;s/\n/\\n/g' || echo "None")
          SECRETS_EXAMPLES=$(head -n 10 results/$DOMAIN/final/all-secrets.txt | sed ':a;N;$!ba;s/\n/\\n/g' || echo "None")
          
          PAYLOAD=$(jq -n \
            --arg title "Bug Bounty Scan Results for $DOMAIN" \
            --arg new_count "$NEW" \
            --arg removed_count "$REMOVED" \
            --arg sensitive_count "$SENSITIVE" \
            --arg secrets_count "$SECRETS" \
            --arg new_examples "$NEW_EXAMPLES" \
            --arg sensitive_examples "$SENSITIVE_EXAMPLES" \
            --arg secrets_examples "$SECRETS_EXAMPLES" \
            '{
              "embeds": [{
                "title": $title,
                "color": 16753920,
                "fields": [
                  {"name":"New endpoints","value":$new_count,"inline":true},
                  {"name":"Removed endpoints","value":$removed_count,"inline":true},
                  {"name":"Sensitive files","value":$sensitive_count,"inline":true},
                  {"name":"Potential secrets","value":$secrets_count,"inline":true},
                  {"name":"New examples (first 10)","value":$new_examples,"inline":false},
                  {"name":"Sensitive files (first 10)","value":$sensitive_examples,"inline":false},
                  {"name":"Potential secrets (first 10)","value":$secrets_examples,"inline":false}
                ],
                "footer": {"text":"Bug Bounty Hunter Suite"}
              }]
            }')
          
          if [ -n "${WEBHOOK_URL:-}" ]; then
            curl -H "Content-Type: application/json" -d "$PAYLOAD" "$WEBHOOK_URL" || true
          else
            echo "[!] No WEBHOOK_URL secret configured; skipping notification."
          fi
